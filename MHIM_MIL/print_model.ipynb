{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/admin/.pyenv/versions/3.10.13/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import torch\n",
    "from copy import deepcopy\n",
    "import torch.nn as nn\n",
    "# from dataloader import *\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler\n",
    "import argparse, os\n",
    "# from modules import attmil,clam,mhim,dsmil,transmil,mean_max\n",
    "from modules import transmil, attmil, dsmil, mhim\n",
    "from torch.nn.functional import one_hot\n",
    "from torch.cuda.amp import GradScaler\n",
    "from contextlib import suppress\n",
    "import time\n",
    "\n",
    "from timm.utils import AverageMeter, dispatch_clip_grad\n",
    "from timm.models import  model_parameters\n",
    "from collections import OrderedDict\n",
    "\n",
    "from utils import *\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "# from tqdm.notebook import tqdm\n",
    "from tqdm import tqdm\n",
    "# import cv2\n",
    "# import torch.nn.functional as F\n",
    "# from torchvision import transforms\n",
    "# from torch.utils.data import Dataset, DataLoader\n",
    "# from torch.optim.lr_scheduler import _LRScheduler\n",
    "# from torchvision.transforms.functional import to_pil_image\n",
    "import albumentations as A\n",
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "from PIL import Image\n",
    "import torchstain\n",
    "import warnings\n",
    "import pickle\n",
    "import gc\n",
    "import random\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(action='ignore')\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "# model = mhim.MHIM(select_mask=False, n_classes=args.n_classes, act=args.act, head=args.n_heads, da_act=args.da_act, baseline=args.baseline).to(device)\n",
    "model = mhim.MHIM(select_mask=False, n_classes=2, act='relu', head=8, da_act='relu', baseline='selfattn').to(device)\n",
    "model_tea = deepcopy(model)\n",
    "\n",
    "# _str = 'fold_{fold}_model_best_auc.pt'.format(fold=args.fold_num) # fold=k\n",
    "_str = 'fold_{fold}_model_best_auc.pt'.format(fold=2) # fold=k\n",
    "_teacher_init = os.path.join('/data/notebook/hyena/modules/init_ckp/c16_3fold_init_transmil_seed2021',_str)\n",
    "\n",
    "# pre_dict = torch.load(_teacher_init)\n",
    "# # print(pre_dict)\n",
    "# info = model_tea.load_state_dict(pre_dict,strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import argparse\n",
    "\n",
    "# parser = argparse.ArgumentParser(description='MIL Training Script')\n",
    "# parser.add_argument('--model', default='mhim', type=str, help='Model name') # mhim\n",
    "# parser.add_argument('--no_tea_init', action='store_true', help='Without teacher initialization')\n",
    "# parser.add_argument('--init_stu_type', default='none', type=str, help='Student initialization [none, fc, all]') # none\n",
    "# parser.add_argument('--tea_type', default='same', type=str, help='[none, same]') # none\n",
    "# parser.add_argument('--no_log', action='store_true', help='Without log') ## store_true\n",
    "\n",
    "model_type = 'mhim'\n",
    "no_tea_init = 'False'\n",
    "init_stu_type = 'none'\n",
    "tea_type = 'same'\n",
    "no_log = 'False'\n",
    "\n",
    "# args = parser.parse_args()\n",
    "\n",
    "if init_stu_type != 'none':\n",
    "    if not no_log:\n",
    "        print('######### Model Initializing.....')\n",
    "    pre_dict = torch.load(_teacher_init)\n",
    "    new_state_dict ={}\n",
    "    if init_stu_type == 'fc':\n",
    "    # only patch_to_emb\n",
    "        for _k, v in pre_dict.items():\n",
    "            _k = _k.replace('patch_to_emb.','') if 'patch_to_emb' in _k else _k\n",
    "            new_state_dict[_k]=v\n",
    "        info = model.patch_to_emb.load_state_dict(new_state_dict,strict=False)\n",
    "    else:\n",
    "    # init all\n",
    "        info = model.load_state_dict(pre_dict,strict=False)\n",
    "    if not no_log:\n",
    "        print(info)\n",
    "\n",
    "# teacher model\n",
    "if model_type == 'mhim':\n",
    "    model_tea = deepcopy(model)\n",
    "    if not no_tea_init and tea_type != 'same':\n",
    "        if not no_log:\n",
    "            print('######### Teacher Initializing.....')\n",
    "            pre_dict = torch.load(_teacher_init)\n",
    "            # print(pre_dict)\n",
    "            info = model_tea.load_state_dict(pre_dict,strict=False)\n",
    "        try:\n",
    "            pre_dict = torch.load(_teacher_init)\n",
    "            info = model_tea.load_state_dict(pre_dict,strict=False)\n",
    "            if not no_log:\n",
    "                print(info)\n",
    "        except:\n",
    "            if not no_log:\n",
    "                print('########## Init Error')\n",
    "    if tea_type == 'same':\n",
    "        model_tea = model\n",
    "else:\n",
    "    model_tea = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MHIM(\n",
      "  (dp): Dropout(p=0.25, inplace=False)\n",
      "  (patch_to_emb): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "  )\n",
      "  (online_encoder): SAttention(\n",
      "    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "    (layer1): TransLayer(\n",
      "      (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): NystromAttention(\n",
      "        (to_qkv): Linear(in_features=512, out_features=1536, bias=False)\n",
      "        (to_out): Sequential(\n",
      "          (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (1): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (res_conv): Conv2d(8, 8, kernel_size=(33, 1), stride=(1, 1), padding=(16, 0), groups=8, bias=False)\n",
      "      )\n",
      "    )\n",
      "    (layer2): TransLayer(\n",
      "      (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): NystromAttention(\n",
      "        (to_qkv): Linear(in_features=512, out_features=1536, bias=False)\n",
      "        (to_out): Sequential(\n",
      "          (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (1): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (res_conv): Conv2d(8, 8, kernel_size=(33, 1), stride=(1, 1), padding=(16, 0), groups=8, bias=False)\n",
      "      )\n",
      "    )\n",
      "    (pos_embedding): PPEG(\n",
      "      (proj): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
      "      (proj1): Conv2d(512, 512, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=512)\n",
      "      (proj2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n",
      "    )\n",
      "  )\n",
      "  (predictor): Linear(in_features=512, out_features=2, bias=True)\n",
      "  (cl_loss): SoftTargetCrossEntropy_v2()\n",
      "  (predictor_cl): Identity()\n",
      "  (target_predictor): Identity()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model_tea)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mhim",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
